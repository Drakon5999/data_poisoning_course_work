%!TEX root = ../data_poisoning.tex
\phantomsection
\section*{Введение}
\addcontentsline{toc}{section}{Введение}
Системы моделей машинного обучения быстро увеличиваются в размерах, приобретают новые возможности и все чаще используются в условиях повышенной ответственности. Как и в случае с другими технологиями, безопасность для моделей машинного обучения должна быть одним из основных исследовательских приоритетов.~\cite{hendrycks_unsolved_2021}

Зачастую сложные обученные модели плохо поддаются интерпретации, и, в большой степени, представляют собой чёрный ящик.
Это свойство делает их уязвимыми к различным отравляющим атакам, поскольку проверка корректности работы модели на всём пространстве признаков зачастую является практически невыполнимой задачей.\cite{chen_detecting_2018}

В данной работе в основном рассмотрены методы отравления обучающих данных на примере моделей для классификации изображений. Однако идеи данных методов также могут быть применены для других модальностей данных и задач машинного обучения.


\subsection*{Обозначения}
\begin{itemize}
    \item ${X_b}$ – множество чистых изображений;
    \item ${X_p}$ – множество отравленных изображений;
    \item ${x_{b} \in X_b}$ – чистое изображение;
    \item ${x_{p} \in X_p}$ – отравленное изображение;
    \item $f(x)$ – функция извлечения признаков.
\end{itemize}
