%!TEX root = ../data_poisoning.tex
\phantomsection
\section*{Вывод}
\addcontentsline{toc}{section}{Вывод}
Обзор современных методов атак отравлением обучающих данных на нейросетевые модели показывает, что на данный момент такие атаки активно развиваются и представляют собой реальную угрозу ввиду своей эффективности в определенных конфигурациях. Например, при определённых условиях, при отравлении 1\% обучающего набора данных можно достигнуть поставленной цели на более чем 90\% моделей обученных на этих данных.


Тут выводы о практических результатах.