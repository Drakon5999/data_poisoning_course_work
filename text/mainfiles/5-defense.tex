%!TEX root = ../data_poisoning.tex
\section{Методы защиты}
\subsection*{Поиск отравленных примеров при помощи кластеризации}
Идея метода детектирования: провести тренировочный набор данных через
обученную нейронную сеть и набор векторов активаций последнего скрытого
слоя. Этот набор разбивается по классам и в каждом классе отдельно хорошо
разделяется на кластеры соответствующие отравленным и чистым данным.
При этом перед кластеризацией используется PCA с проекцией на 3 первых
компоненты.

Самым эффективным методом кластеризации выбран k-means с $k=2$ Далее для проверки, что данные отравлены мы можем исключить какой-нибудь
кластер из обучающего множества и затем провести его через новый классификатор. Если большая доля примеров попадает в один конкретный неверный
класс, то это подозрительно и требует проверки.
Также когда отравленных данных $p\%$, то размер кластера, соответствующего им будет составлять примерно $p\%$. Если же отравленных данных нет,
то размеры кластеров будут примерно равны. С той же целью можно использовать silhouette score.
Также мы можем составить усреднённый пример из кластера и проверив его вручную сделать вывод о наличии отравления: например триггеры атак внедрения backdoor могут сильно выделяться.
Для исправления модели предлагается изменить метки отравленным
классам и дообучить модель на исправленных данных.

\subsection*{Поиск отравленных примеров при помощи Auto Encoder моделей}
Модель Auto Encoder можно использовать для детектирования сильных аномалий, потому что он не может их точно восстановить. Но для этого Auto Encoder должен Быть обучен на доверенном наборе данных.

Однако в чистом виде этот метод обладает несколькими недостатками:
\begin{itemize}
    \item Отравление часто происходит малыми изменениями, что может быть незаметным для Auto Encoder модели;
    \item Такой подход не анализирует метки классов, а они могут быть изменены;
    \item Требование доверенного набора данных накладывает существенные ограничения на применимость этого метода.
\end{itemize}

Для решения этих проблем был разработан метод под названием
"Classification Auto-Encoder based detector $+$".

%TODO: описать метод подробнее

\subsection*{Сертификация робастности модели}

Для некоторых моделей возможно доказать их устойчивость к отравлениям. Например для Bagging классификатора.
Обучающие подмножества для каждого базового классификатора выбираются случайно. Процесс обучения тоже носит стохастический характер.

Теоретическое определение вероятности того, что предсказанная метка при отравлении не изменится, получается при помощи леммы Неймана-Пирсона.
Определяется нижняя граница вероятности наиболее вероятного
предсказания и верхняя граница вероятности второго по вероятности предсказания для того чтобы при отравлении $r$ примеров не произошло изменений в предсказании.

$r$ получается как решение оптимизационной задачи.

%TODO описать задачу


\subsection*{Метод очистки отравленной модели GangSweep}
Ключевым понятием этого метода является "изменяющая маска" – минимальное попиксельное изменение изображения, при котором модель начинает классифицировать изображение из класса $C$ как изображение из класса $C'$.

По результатам экспериментов, внедрение backdoor слабо изменяет дисперсию в пространстве признаков, но сильно изменяет расстояние.

Состязательные нейросети при генерации изменяющей маски очень хорошо перебирают
пространство вокруг объекта. В методе используется состязательная сеть
для генерации изменяющей маски от каждого класса к каждому.
Если в модель внедрён Backdoor, то для различных изображений изменяющие маски к целевому классу будут очень близки.

Отфильтровать изменяющие маски для последующего анализа можно посредством алгоритма z-score, для того чтобы выделить те, которые слабо изменяют дисперсию, но сильно изменяют расстояние в пространстве признаков.

% TODO: добавить изображения
