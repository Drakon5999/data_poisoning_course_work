%!TEX root = ../data_poisoning.tex
\phantomsection
\section*{Введение}
\addcontentsline{toc}{section}{Введение}
Системы моделей машинного обучения быстро увеличиваются в размерах, приобретают новые возможности и все чаще используются в условиях повышенной ответственности. Как и в случае с другими технологиями, безопасность для моделей машинного обучения должна быть одним из основных исследовательских приоритетов.

Зачастую сложные обученные модели плохо поддаются интерпретации, и, в большой степени, представляют собой чёрный ящик.
Это свойство делает их уязвимыми к различным отравляющим атакам, поскольку проверка корректности работы модели на всём пространстве признаков зачастую является практически невыполнимой задачей.~[1]

В данной работе в основном рассмотрены методы отравления обучающих данных на примере моделей для классификации изображений. Однако идеи данных методов также могут быть применены для других модальностей данных и задач машинного обучения.


