%!TEX root = ../data_poisoning.tex
\section{Атаки отравлением данных}
\subsection*{Цель}
Изменить набор обучающий данных $D$ таким образом, чтобы целевое изображение $x_t \in D$, имеющее класс $C$ неверно классифицировалось в целевой класс $C'$.
При этом нужно оказать как можно меньшее влияние на классификацию остальных изображений.

\subsection*{Дополнительные обозначения}
\begin{itemize}
    \item ${x_{b}}$ – чистое изображение;
    \item ${x_{p} \in X_p}$ – отравленное изображение;
    \item $f(x)$ – функция извлечения признаков;
\end{itemize}

\subsection*{Классические методы отравления данных}

\subsubsection*{Модель угроз}
Атакующий контролирует обучающий данных $D$ и может произвольно менять в нём данные в пределах $J$ примеров.

\subsubsection*{Описание атак}
\textbf{Простая замена метки класса у целевого изображения}
Очень малоэффективная атака при больших объёмах доступных для обучения данных. Зачастую модели обладают хорошей обобщающей способностью и с большой вероятностью не выучат один неверно размеченный пример.

\textbf{Feature Collision}
Вместе с простой заменой метки класса у целевого изображения, мы пытаемся осуществить минимальное изменение изображений из целевого класса $C'$ так чтобы признаки отравленных данных были максимально близки к признакам целевого изображения.

$$x^{j}_{p} = \argmin_x ||f(x)-f(x_t)||_{2}^{2}$$

$$\text{при условии, что~} ||x_{p}^{j} - x_{b}^{j}||_{\infty} \leq \epsilon$$


\textbf{Convex Polytope}
Вместе с простой заменой метки класса у целевого изображения, мы пытаемся сделать так, чтобы признаки целевого изображения представлялись линейной комбинацией отравленных данных из класса $C'$

$$X_p = \argmin_{c_j, x^j} ||f(x_t) - \sum_{j=1}^{J} c_j f(x^j){||}_{2}^{2}$$
$$\text{при условии~} \sum_{j=1}^{J} c_j = 1$$
$$\text{и~} c_j \geq 0~\forall j$$
$$\text{и~} ||x_{p}^{j} - x_{b}^{j}||_{\infty} \leq \epsilon$$


\textbf{Witches' Brew via gradient matching}
При данной атаке нам дополнительно известна архитектура обучаемой модели $F(x, \theta)$ и её функция потерь $L(y_{pred}, y_{true})$.
Вместе с простой заменой метки класса у целевого изображения, мы отравляем данные из класса $C'$ так чтобы вектора градиентов функции потерь для них были сонаправлены с вектором градиента функции потерь для целевого изображения.

$$\nabla_{\theta} L(F(x_t, \theta), C') \uparrow\uparrow \nabla_{\theta} L(F(x_{p}^{j}, \theta), C')$$
$$\text{где~} x_{p}^{j} = x_{b}^{j} + \delta_j \text{~и~} ||\delta_j||_{\infty} \leq \epsilon$$

\subsection*{Методы атак без доступа к полному набору обучающих данных}


\subsubsection*{Модель угроз}
В атаках без возможности чтения обучающих данных (также называются атаки методом чёрного ящика) ожидается, что атакующий не имеет доступа к обучающему набору данных, однако знает из какого они распределения и имеет возможность выбирать данные из этого распределения неограниченно, либо по определённым правилам. Также атакующий знает структуру обучаемой модели, её функцию потерь и имеет возможность внедрять отравленные данные в обучающий набор данных для конечной модели.

\textbf{Subpopulation Attack}
Идея заключается в следующем: мы выбираем большое количество данных из известного нам распределения исходных данных. Затем проводим кластеризацию.

Кластеризацию можно производить:
\begin{itemize}
    \item Вручную по высокоуровнему признаку: например наличия на картинке какого-либо элемента
    \item Автоматически путём предварительной фильтрации и кластеризации либо оригинальных изображений, либо извлечённых признаков
\end{itemize}

Кластеризацию следует производить таким образом, чтобы кластер с целевым изображением имел размер не более чем $J$. Затем мы меняем метку на $C'$ для всего кластера и отправляем получившиеся данные на обучение модели.

% TODO: описать оптимизацию атаки

\textbf{Online Learning Attack}
В атаке предполагается что модель обучается в режиме реального времени и атакующему известно текущие веса модели. В каждый момент времени атакующий получает обучающий пример и у него есть два варианта действия:
\begin{itemize}
    \item Передать пример модели
    \item Модифицировать пример и передать модели
\end{itemize}

При этом атакующий не может менять принятого ранее решения.

Задачу можно переформулировать в виде марковского процесса принятия решений, введя функцию штрафов $g(s_T, x_T)$, где $s_T$ – текущее состояние, описываемое положением в марковском процессе и весами модели, $x_T$ – пример, доступный в момент времени $T$.

\noindentТогда ожидаемый штраф для атакующего будет составлять:
$$V_{\mathcal{M}}^{\phi}(\mathbf{s}):=\left.\mathbb{E}_{\mathcal{M}} \sum_{T=0}^{\infty} \gamma^{T} g\left(\mathbf{s}_{T}, \phi\left(\mathbf{s}_{T}\right)\right)\right|_{\mathbf{s0}=\mathbf{s}}$$
Где $\gamma$ – дисконтирующий множитель, введённый для решения проблемы с неограниченностью процесса по времени.


\noindentА оптимальная политика действий:
$$
\phi* = \argmin_{\phi} \mathbb{E}_{\mathbf{S} \sim \mu_{0}} V_{\mathcal{M}}^{\phi}(\mathbf{s})
$$
