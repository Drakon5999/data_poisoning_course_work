
@article{gorbunov_secure_2021,
	title = {Secure {Distributed} {Training} at {Scale}},
	url = {http://arxiv.org/abs/2106.11257},
	abstract = {Some of the hardest problems in deep learning can be solved via pooling together computational resources of many independent parties, as is the case for scientific collaborations and volunteer computing. Unfortunately, any single participant in such systems can jeopardize the entire training run by sending incorrect updates, whether deliberately or by mistake. Training in presence of such peers requires specialized distributed training algorithms with Byzantine tolerance. These algorithms often sacrifice efficiency by introducing redundant communication or passing all updates through a trusted server. As a result, it can be infeasible to apply such algorithms to large-scale distributed deep learning, where models can have billions of parameters. In this work, we propose a novel protocol for secure (Byzantine-tolerant) decentralized training that emphasizes communication efficiency. We rigorously analyze this protocol: in particular, we provide theoretical bounds for its resistance against Byzantine and Sybil attacks and show that it has a marginal communication overhead. To demonstrate its practical effectiveness, we conduct large-scale experiments on image classification and language modeling in presence of Byzantine attackers.},
	urldate = {2021-11-05},
	journal = {arXiv:2106.11257 [cs, math]},
	author = {Gorbunov, Eduard and Borzunov, Alexander and Diskin, Michael and Ryabinin, Max},
	month = oct,
	year = {2021},
	note = {arXiv: 2106.11257},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:C\:\\Users\\ar7is7\\Zotero\\storage\\8USQNQ9Y\\Gorbunov et al. - 2021 - Secure Distributed Training at Scale.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ar7is7\\Zotero\\storage\\AEHIDYCW\\2106.html:text/html},
}

@article{schwarzschild_just_2021,
	title = {Just {How} {Toxic} is {Data} {Poisoning}? {A} {Unified} {Benchmark} for {Backdoor} and {Data} {Poisoning} {Attacks}},
	shorttitle = {Just {How} {Toxic} is {Data} {Poisoning}?},
	url = {http://arxiv.org/abs/2006.12557},
	abstract = {Data poisoning and backdoor attacks manipulate training data in order to cause models to fail during inference. A recent survey of industry practitioners found that data poisoning is the number one concern among threats ranging from model stealing to adversarial attacks. However, it remains unclear exactly how dangerous poisoning methods are and which ones are more effective considering that these methods, even ones with identical objectives, have not been tested in consistent or realistic settings. We observe that data poisoning and backdoor attacks are highly sensitive to variations in the testing setup. Moreover, we find that existing methods may not generalize to realistic settings. While these existing works serve as valuable prototypes for data poisoning, we apply rigorous tests to determine the extent to which we should fear them. In order to promote fair comparison in future work, we develop standardized benchmarks for data poisoning and backdoor attacks.},
	urldate = {2021-11-05},
	journal = {arXiv:2006.12557 [cs, stat]},
	author = {Schwarzschild, Avi and Goldblum, Micah and Gupta, Arjun and Dickerson, John P. and Goldstein, Tom},
	month = jun,
	year = {2021},
	note = {arXiv: 2006.12557},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computers and Society, Computer Science - Cryptography and Security, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ar7is7\\Zotero\\storage\\AFINERFV\\Schwarzschild et al. - 2021 - Just How Toxic is Data Poisoning A Unified Benchm.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ar7is7\\Zotero\\storage\\5AP9HCRS\\2006.html:text/html},
}

@article{geiping_witches_2021,
	title = {Witches' {Brew}: {Industrial} {Scale} {Data} {Poisoning} via {Gradient} {Matching}},
	shorttitle = {Witches' {Brew}},
	url = {http://arxiv.org/abs/2009.02276},
	abstract = {Data Poisoning attacks modify training data to maliciously control a model trained on such data. In this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a particularly malicious poisoning attack that is both "from scratch" and "clean label", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. Previous poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets. The central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset. Finally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems.},
	urldate = {2021-11-05},
	journal = {arXiv:2009.02276 [cs]},
	author = {Geiping, Jonas and Fowl, Liam and Huang, W. Ronny and Czaja, Wojciech and Taylor, Gavin and Moeller, Michael and Goldstein, Tom},
	month = may,
	year = {2021},
	note = {arXiv: 2009.02276},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ar7is7\\Zotero\\storage\\BE5BJIML\\Geiping et al. - 2021 - Witches' Brew Industrial Scale Data Poisoning via.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ar7is7\\Zotero\\storage\\KNSCV2DS\\2009.html:text/html},
}

@article{hendrycks_unsolved_2021,
	title = {Unsolved {Problems} in {ML} {Safety}},
	url = {http://arxiv.org/abs/2109.13916},
	abstract = {Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards ("Robustness"), identifying hazards ("Monitoring"), steering ML systems ("Alignment"), and reducing hazards in deployment ("External Safety"). Throughout, we clarify each problem's motivation and provide concrete research directions.},
	urldate = {2021-11-05},
	journal = {arXiv:2109.13916 [cs]},
	author = {Hendrycks, Dan and Carlini, Nicholas and Schulman, John and Steinhardt, Jacob},
	month = oct,
	year = {2021},
	note = {arXiv: 2109.13916},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ar7is7\\Zotero\\storage\\5DVGMTWX\\Hendrycks et al. - 2021 - Unsolved Problems in ML Safety.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ar7is7\\Zotero\\storage\\UL4C8FEG\\2109.html:text/html},
}

@article{chen_detecting_2018,
	title = {Detecting {Backdoor} {Attacks} on {Deep} {Neural} {Networks} by {Activation} {Clustering}},
	url = {http://arxiv.org/abs/1811.03728},
	abstract = {While machine learning (ML) models are being increasingly trusted to make decisions in different and varying areas, the safety of systems using such models has become an increasing concern. In particular, ML models are often trained on data from potentially untrustworthy sources, providing adversaries with the opportunity to manipulate them by inserting carefully crafted samples into the training set. Recent work has shown that this type of attack, called a poisoning attack, allows adversaries to insert backdoors or trojans into the model, enabling malicious behavior with simple external backdoor triggers at inference time and only a blackbox perspective of the model itself. Detecting this type of attack is challenging because the unexpected behavior occurs only when a backdoor trigger, which is known only to the adversary, is present. Model users, either direct users of training data or users of pre-trained model from a catalog, may not guarantee the safe operation of their ML-based system. In this paper, we propose a novel approach to backdoor detection and removal for neural networks. Through extensive experimental results, we demonstrate its effectiveness for neural networks classifying text and images. To the best of our knowledge, this is the first methodology capable of detecting poisonous data crafted to insert backdoors and repairing the model that does not require a verified and trusted dataset.},
	urldate = {2021-11-05},
	journal = {arXiv:1811.03728 [cs, stat]},
	author = {Chen, Bryant and Carvalho, Wilka and Baracaldo, Nathalie and Ludwig, Heiko and Edwards, Benjamin and Lee, Taesung and Molloy, Ian and Srivastava, Biplav},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.03728},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ar7is7\\Zotero\\storage\\3NENSYAQ\\Chen et al. - 2018 - Detecting Backdoor Attacks on Deep Neural Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ar7is7\\Zotero\\storage\\YQSMPXAA\\1811.html:text/html},
}

@article{razmi_classification_2021,
	title = {Classification {Auto}-{Encoder} based {Detector} against {Diverse} {Data} {Poisoning} {Attacks}},
	url = {http://arxiv.org/abs/2108.04206},
	abstract = {Poisoning attacks are a category of adversarial machine learning threats in which an adversary attempts to subvert the outcome of the machine learning systems by injecting crafted data into training data set, thus increasing the machine learning model's test error. The adversary can tamper with the data feature space, data labels, or both, each leading to a different attack strategy with different strengths. Various detection approaches have recently emerged, each focusing on one attack strategy. The Achilles heel of many of these detection approaches is their dependence on having access to a clean, untampered data set. In this paper, we propose CAE, a Classification Auto-Encoder based detector against diverse poisoned data. CAE can detect all forms of poisoning attacks using a combination of reconstruction and classification errors without having any prior knowledge of the attack strategy. We show that an enhanced version of CAE (called CAE+) does not have to employ a clean data set to train the defense model. Our experimental results on three real datasets MNIST, Fashion-MNIST and CIFAR demonstrate that our proposed method can maintain its functionality under up to 30\% contaminated data and help the defended SVM classifier to regain its best accuracy.},
	urldate = {2021-11-13},
	journal = {arXiv:2108.04206 [cs]},
	author = {Razmi, Fereshteh and Xiong, Li},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.04206},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ar7is7\\Zotero\\storage\\CM4XA824\\Razmi and Xiong - 2021 - Classification Auto-Encoder based Detector against.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ar7is7\\Zotero\\storage\\LBWFELAS\\2108.html:text/html},
}

@article{zhao_clean-label_2020,
	title = {Clean-{Label} {Backdoor} {Attacks} on {Video} {Recognition} {Models}},
	url = {http://arxiv.org/abs/2003.03030},
	abstract = {Deep neural networks (DNNs) are vulnerable to backdoor attacks which can hide backdoor triggers in DNNs by poisoning training data. A backdoored model behaves normally on clean test images, yet consistently predicts a particular target class for any test examples that contain the trigger pattern. As such, backdoor attacks are hard to detect, and have raised severe security concerns in real-world applications. Thus far, backdoor research has mostly been conducted in the image domain with image classification models. In this paper, we show that existing image backdoor attacks are far less effective on videos, and outline 4 strict conditions where existing attacks are likely to fail: 1) scenarios with more input dimensions (eg. videos), 2) scenarios with high resolution, 3) scenarios with a large number of classes and few examples per class (a "sparse dataset"), and 4) attacks with access to correct labels (eg. clean-label attacks). We propose the use of a universal adversarial trigger as the backdoor trigger to attack video recognition models, a situation where backdoor attacks are likely to be challenged by the above 4 strict conditions. We show on benchmark video datasets that our proposed backdoor attack can manipulate state-of-the-art video models with high success rates by poisoning only a small proportion of training data (without changing the labels). We also show that our proposed backdoor attack is resistant to state-of-the-art backdoor defense/detection methods, and can even be applied to improve image backdoor attacks. Our proposed video backdoor attack not only serves as a strong baseline for improving the robustness of video models, but also provides a new perspective for more understanding more powerful backdoor attacks.},
	urldate = {2021-11-21},
	journal = {arXiv:2003.03030 [cs]},
	author = {Zhao, Shihao and Ma, Xingjun and Zheng, Xiang and Bailey, James and Chen, Jingjing and Jiang, Yu-Gang},
	month = jun,
	year = {2020},
	note = {arXiv: 2003.03030},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\ar7is7\\Zotero\\storage\\PYAUR46C\\Zhao et al. - 2020 - Clean-Label Backdoor Attacks on Video Recognition .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ar7is7\\Zotero\\storage\\5J6YMX9X\\2003.html:text/html},
}

@article{turner_clean-label_nodate,
	title = {Clean-{Label} {Backdoor} {Attacks}},
	abstract = {Deep neural networks have been recently demonstrated to be vulnerable to backdoor attacks. Speciﬁcally, by altering a small set of training examples, an adversary is able to install a backdoor that can be used during inference to fully control the model’s behavior. While the attack is very powerful, it crucially relies on the adversary being able to introduce arbitrary, often clearly mislabeled, inputs to the training set and can thus be detected even by fairly rudimentary data ﬁltering. In this paper, we introduce a new approach to executing backdoor attacks, utilizing adversarial examples and GAN-generated data. The key feature is that the resulting poisoned inputs appear to be consistent with their label and thus seem benign even upon human inspection.},
	language = {en},
	author = {Turner, Alexander and Tsipras, Dimitris and Mądry, Aleksander},
	pages = {21},
	file = {Turner et al. - Clean-Label Backdoor Attacks.pdf:C\:\\Users\\ar7is7\\Zotero\\storage\\PCP765J4\\Turner et al. - Clean-Label Backdoor Attacks.pdf:application/pdf},
}

@article{wang_backdoorl_2021,
	title = {{BACKDOORL}: {Backdoor} {Attack} against {Competitive} {Reinforcement} {Learning}},
	shorttitle = {{BACKDOORL}},
	url = {http://arxiv.org/abs/2105.00579},
	abstract = {Recent research has confirmed the feasibility of backdoor attacks in deep reinforcement learning (RL) systems. However, the existing attacks require the ability to arbitrarily modify an agent's observation, constraining the application scope to simple RL systems such as Atari games. In this paper, we migrate backdoor attacks to more complex RL systems involving multiple agents and explore the possibility of triggering the backdoor without directly manipulating the agent's observation. As a proof of concept, we demonstrate that an adversary agent can trigger the backdoor of the victim agent with its own action in two-player competitive RL systems. We prototype and evaluate BACKDOORL in four competitive environments. The results show that when the backdoor is activated, the winning rate of the victim drops by 17\% to 37\% compared to when not activated.},
	urldate = {2021-11-21},
	journal = {arXiv:2105.00579 [cs]},
	author = {Wang, Lun and Javed, Zaynah and Wu, Xian and Guo, Wenbo and Xing, Xinyu and Song, Dawn},
	month = may,
	year = {2021},
	note = {arXiv: 2105.00579},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:C\:\\Users\\ar7is7\\Zotero\\storage\\LRUQHJZ4\\Wang et al. - 2021 - BACKDOORL Backdoor Attack against Competitive Rei.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ar7is7\\Zotero\\storage\\ID9LTCP5\\2105.html:text/html},
}

@article{salem_dont_2020,
	title = {Don't {Trigger} {Me}! {A} {Triggerless} {Backdoor} {Attack} {Against} {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2010.03282},
	abstract = {Backdoor attack against deep neural networks is currently being profoundly investigated due to its severe security consequences. Current state-of-the-art backdoor attacks require the adversary to modify the input, usually by adding a trigger to it, for the target model to activate the backdoor. This added trigger not only increases the difficulty of launching the backdoor attack in the physical world, but also can be easily detected by multiple defense mechanisms. In this paper, we present the first triggerless backdoor attack against deep neural networks, where the adversary does not need to modify the input for triggering the backdoor. Our attack is based on the dropout technique. Concretely, we associate a set of target neurons that are dropped out during model training with the target label. In the prediction phase, the model will output the target label when the target neurons are dropped again, i.e., the backdoor attack is launched. This triggerless feature of our attack makes it practical in the physical world. Extensive experiments show that our triggerless backdoor attack achieves a perfect attack success rate with a negligible damage to the model's utility.},
	urldate = {2021-11-21},
	journal = {arXiv:2010.03282 [cs]},
	author = {Salem, Ahmed and Backes, Michael and Zhang, Yang},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.03282},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:C\:\\Users\\ar7is7\\Zotero\\storage\\UG48Z98W\\Salem et al. - 2020 - Don't Trigger Me! A Triggerless Backdoor Attack Ag.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ar7is7\\Zotero\\storage\\CZCRW4KG\\2010.html:text/html},
}

@inproceedings{zhu_gangsweep_2020,
	address = {New York, NY, USA},
	series = {{MM} '20},
	title = {{GangSweep}: {Sweep} out {Neural} {Backdoors} by {GAN}},
	isbn = {978-1-4503-7988-5},
	shorttitle = {{GangSweep}},
	url = {https://doi.org/10.1145/3394171.3413546},
	doi = {10.1145/3394171.3413546},
	abstract = {This work proposes GangSweep, a new backdoor detection framework that leverages the super reconstructive power of Generative Adversarial Networks (GAN) to detect and ''sweep out'' neural backdoors. It is motivated by a series of intriguing empirical investigations, revealing that the perturbation masks generated by GAN are persistent and exhibit interesting statistical properties with low shifting variance and large shifting distance in feature space. Compared with the previous solutions, the proposed approach eliminates the reliance on the access to training data, and shows a high degree of robustness and efficiency for detecting and mitigating a wide range of backdoored models with various settings. Moreover, this is the first work that successfully leverages generative networks to defend against advanced neural backdoors with multiple triggers and their polymorphic forms.},
	urldate = {2021-11-28},
	booktitle = {Proceedings of the 28th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Zhu, Liuwan and Ning, Rui and Wang, Cong and Xin, Chunsheng and Wu, Hongyi},
	month = oct,
	year = {2020},
	keywords = {deep neural network, model verification, neural backdoor},
	pages = {3173--3181},
	file = {Full Text PDF:C\:\\Users\\ar7is7\\Zotero\\storage\\K4T6B8UH\\Zhu et al. - 2020 - GangSweep Sweep out Neural Backdoors by GAN.pdf:application/pdf},
}

@inproceedings{wang_neural_2019,
	address = {San Francisco, CA, USA},
	title = {Neural {Cleanse}: {Identifying} and {Mitigating} {Backdoor} {Attacks} in {Neural} {Networks}},
	isbn = {978-1-5386-6660-9},
	shorttitle = {Neural {Cleanse}},
	url = {https://ieeexplore.ieee.org/document/8835365/},
	doi = {10.1109/SP.2019.00031},
	abstract = {Lack of transparency in deep neural networks (DNNs) make them susceptible to backdoor attacks, where hidden associations or triggers override normal classiﬁcation to produce unexpected results. For example, a model with a backdoor always identiﬁes a face as Bill Gates if a speciﬁc symbol is present in the input. Backdoors can stay hidden indeﬁnitely until activated by an input, and present a serious security risk to many security or safety related applications, e.g., biometric authentication systems or self-driving cars.},
	language = {en},
	urldate = {2021-12-11},
	booktitle = {2019 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	publisher = {IEEE},
	author = {Wang, Bolun and Yao, Yuanshun and Shan, Shawn and Li, Huiying and Viswanath, Bimal and Zheng, Haitao and Zhao, Ben Y.},
	month = may,
	year = {2019},
	pages = {707--723},
	file = {Wang et al. - 2019 - Neural Cleanse Identifying and Mitigating Backdoo.pdf:C\:\\Users\\ar7is7\\Zotero\\storage\\4XN3N6G5\\Wang et al. - 2019 - Neural Cleanse Identifying and Mitigating Backdoo.pdf:application/pdf},
}

@article{cheng_deep_2021,
	title = {Deep {Feature} {Space} {Trojan} {Attack} of {Neural} {Networks} by {Controlled} {Detoxification}},
	url = {http://arxiv.org/abs/2012.11212},
	abstract = {Trojan (backdoor) attack is a form of adversarial attack on deep neural networks where the attacker provides victims with a model trained/retrained on malicious data. The backdoor can be activated when a normal input is stamped with a certain pattern called trigger, causing misclassification. Many existing trojan attacks have their triggers being input space patches/objects (e.g., a polygon with solid color) or simple input transformations such as Instagram filters. These simple triggers are susceptible to recent backdoor detection algorithms. We propose a novel deep feature space trojan attack with five characteristics: effectiveness, stealthiness, controllability, robustness and reliance on deep features. We conduct extensive experiments on 9 image classifiers on various datasets including ImageNet to demonstrate these properties and show that our attack can evade state-of-the-art defense.},
	urldate = {2021-12-12},
	journal = {arXiv:2012.11212 [cs]},
	author = {Cheng, Siyuan and Liu, Yingqi and Ma, Shiqing and Zhang, Xiangyu},
	month = jan,
	year = {2021},
	note = {arXiv: 2012.11212},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ar7is7\\Zotero\\storage\\BGTV4PSI\\Cheng et al. - 2021 - Deep Feature Space Trojan Attack of Neural Network.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ar7is7\\Zotero\\storage\\R28YLECQ\\2012.html:text/html},
}

@article{li_rethinking_2021,
	title = {Rethinking the {Trigger} of {Backdoor} {Attack}},
	url = {http://arxiv.org/abs/2004.04692},
	abstract = {Backdoor attack intends to inject hidden backdoor into the deep neural networks (DNNs), such that the prediction of the infected model will be maliciously changed if the hidden backdoor is activated by the attacker-defined trigger, while it performs well on benign samples. Currently, most of existing backdoor attacks adopted the setting of {\textbackslash}emph\{static\} trigger, \$i.e.,\$ triggers across the training and testing images follow the same appearance and are located in the same area. In this paper, we revisit this attack paradigm by analyzing the characteristics of the static trigger. We demonstrate that such an attack paradigm is vulnerable when the trigger in testing images is not consistent with the one used for training. We further explore how to utilize this property for backdoor defense, and discuss how to alleviate such vulnerability of existing attacks.},
	urldate = {2021-12-13},
	journal = {arXiv:2004.04692 [cs]},
	author = {Li, Yiming and Zhai, Tongqing and Wu, Baoyuan and Jiang, Yong and Li, Zhifeng and Xia, Shutao},
	month = jan,
	year = {2021},
	note = {arXiv: 2004.04692},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:C\:\\Users\\ar7is7\\Zotero\\storage\\354G7BGS\\Li et al. - 2021 - Rethinking the Trigger of Backdoor Attack.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ar7is7\\Zotero\\storage\\8ZXHVD3N\\2004.html:text/html},
}

@inproceedings{jagielski_subpopulation_2021,
	address = {New York, NY, USA},
	series = {{CCS} '21},
	title = {Subpopulation {Data} {Poisoning} {Attacks}},
	isbn = {978-1-4503-8454-4},
	url = {https://doi.org/10.1145/3460120.3485368},
	doi = {10.1145/3460120.3485368},
	abstract = {Machine learning systems are deployed in critical settings, but they might fail in unexpected ways, impacting the accuracy of their predictions. Poisoning attacks against machine learning induce adversarial modification of data used by a machine learning algorithm to selectively change its output when it is deployed. In this work, we introduce a novel data poisoning attack called a subpopulation attack, which is particularly relevant when datasets are large and diverse. We design a modular framework for subpopulation attacks, instantiate it with different building blocks, and show that the attacks are effective for a variety of datasets and machine learning models. We further optimize the attacks in continuous domains using influence functions and gradient optimization methods. Compared to existing backdoor poisoning attacks, subpopulation attacks have the advantage of inducing misclassification in naturally distributed data points at inference time, making the attacks extremely stealthy. We also show that our attack strategy can be used to improve upon existing targeted attacks. We prove that, under some assumptions, subpopulation attacks are impossible to defend against, and empirically demonstrate the limitations of existing defenses against our attacks, highlighting the difficulty of protecting machine learning against this threat.},
	urldate = {2021-12-14},
	booktitle = {Proceedings of the 2021 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Jagielski, Matthew and Severi, Giorgio and Pousette Harger, Niklas and Oprea, Alina},
	month = nov,
	year = {2021},
	keywords = {adversarial machine learning, fairness, poisoning attacks},
	pages = {3104--3122},
	file = {Full Text PDF:C\:\\Users\\ar7is7\\Zotero\\storage\\YM5LKSZ9\\Jagielski et al. - 2021 - Subpopulation Data Poisoning Attacks.pdf:application/pdf},
}

@article{liu_data_2021,
	title = {Data poisoning against information-theoretic feature selection},
	volume = {573},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521005090},
	doi = {10.1016/j.ins.2021.05.049},
	abstract = {A typical assumption made in machine learning is that a learning model does not consider an adversary’s existence that can subvert a classifier’s objective. As a result, machine learning pipelines exhibit vulnerabilities in an adversarial environment. Feature Selection (FS) is an essential preprocessing stage in data analytics and has been widely used in security-sensitive machine learning applications; however, FS research in adversarial machine learning has been largely overlooked. Recently, empirical works demonstrated that the FS is also vulnerable in an adversarial environment. In the past decade, although the research community has made extensive efforts to promote the classifiers’ robustness and develop countermeasures against adversaries, only a few contributions investigated FS’s behavior in a malicious environment. Given that machine learning pipelines increasingly rely on FS to combat the “curse of dimensionality” and overfitting, insecure FS can be the “Achilles heel” of data pipelines. In this contribution, we explore the weaknesses of information-theoretic FS methods by designing a generic FS poisoning algorithm. We also show the transferability of the proposed poisoning method across seven information-theoretic FS methods. The experiments on 16 benchmark datasets demonstrate the efficacy of our proposed poisoning algorithm and the existence of transferability.},
	language = {en},
	urldate = {2021-12-15},
	journal = {Information Sciences},
	author = {Liu, Heng and Ditzler, Gregory},
	month = sep,
	year = {2021},
	keywords = {Adversarial learning, Feature selection, Information theory},
	pages = {396--411},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\ar7is7\\Zotero\\storage\\G7K5G6CT\\Liu and Ditzler - 2021 - Data poisoning against information-theoretic featu.pdf:application/pdf},
}

@inproceedings{zhang_online_2020,
	title = {Online {Data} {Poisoning} {Attacks}},
	url = {https://proceedings.mlr.press/v120/zhang20b.html},
	abstract = {We study data poisoning attacks in the online learning setting, where training data arrive sequentially, and the attacker is eavesdropping the data stream and has the ability to contaminate the current data point to affect the online learning process. We formulate the optimal online attack problem as a stochastic optimal control problem, and provide a systematic solution using tools from model predictive control and deep reinforcement learning. We further provide theoretical analysis on the regret suffered by the attacker for not knowing the true data sequence. Experiments validate our control approach in generating near-optimal attacks on both supervised and unsupervised learning tasks.},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {Proceedings of the 2nd {Conference} on {Learning} for {Dynamics} and {Control}},
	publisher = {PMLR},
	author = {Zhang, Xuezhou and Zhu, Xiaojin and Lessard, Laurent},
	month = jul,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {201--210},
	file = {Full Text PDF:C\:\\Users\\ar7is7\\Zotero\\storage\\7LWEIVKV\\Zhang et al. - 2020 - Online Data Poisoning Attacks.pdf:application/pdf},
}

@article{verde_exploring_2021,
	series = {Knowledge-{Based} and {Intelligent} {Information} \& {Engineering} {Systems}: {Proceedings} of the 25th {International} {Conference} {KES2021}},
	title = {Exploring the {Impact} of {Data} {Poisoning} {Attacks} on {Machine} {Learning} {Model} {Reliability}},
	volume = {192},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050921017695},
	doi = {10.1016/j.procs.2021.09.032},
	abstract = {Recent years have seen the widespread adoption of Artificial Intelligence techniques in several domains, including healthcare, justice, assisted driving and Natural Language Processing (NLP) based applications (e.g., the Fake News detection). Those mentioned are just a few examples of some domains that are particularly critical and sensitive to the reliability of the adopted machine learning systems. Therefore, several Artificial Intelligence approaches were adopted as support to realize easy and reliable solutions aimed at improving the early diagnosis, personalized treatment, remote patient monitoring and better decision-making with a consequent reduction of healthcare costs. Recent studies have shown that these techniques are venerable to attacks by adversaries at phases of artificial intelligence. Poisoned data set are the most common attack to the reliability of Artificial Intelligence approaches. Noise, for example, can have a significant impact on the overall performance of a machine learning model. This study discusses the strength of impact of noise on classification algorithms. In detail, the reliability of several machine learning techniques to distinguish correctly pathological and healthy voices by analysing poisoning data was evaluated. Voice samples selected by available database, widely used in research sector, the Saarbruecken Voice Database, were processed and analysed to evaluate the resilience and classification accuracy of these techniques. All analyses are evaluated in terms of accuracy, specificity, sensitivity, F1-score and ROC area.},
	language = {en},
	urldate = {2021-12-19},
	journal = {Procedia Computer Science},
	author = {Verde, Laura and Marulli, Fiammetta and Marrone, Stefano},
	month = jan,
	year = {2021},
	keywords = {Data Poisoning Attacks, Disorders detection, Poisoned Big Data, Reliability, Resilient Machine Learning, Security, Voice quality assessment.},
	pages = {2624--2632},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\ar7is7\\Zotero\\storage\\NLS2QMS3\\Verde et al. - 2021 - Exploring the Impact of Data Poisoning Attacks on .pdf:application/pdf},
}

@article{sun_data_2020,
	title = {Data {Poisoning} {Attacks} on {Federated} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2004.10020},
	abstract = {Federated machine learning which enables resource constrained node devices (e.g., mobile phones and IoT devices) to learn a shared model while keeping the training data local, can provide privacy, security and economic benefits by designing an effective communication protocol. However, the communication protocol amongst different nodes could be exploited by attackers to launch data poisoning attacks, which has been demonstrated as a big threat to most machine learning models. In this paper, we attempt to explore the vulnerability of federated machine learning. More specifically, we focus on attacking a federated multi-task learning framework, which is a federated learning framework via adopting a general multi-task learning framework to handle statistical challenges. We formulate the problem of computing optimal poisoning attacks on federated multi-task learning as a bilevel program that is adaptive to arbitrary choice of target nodes and source attacking nodes. Then we propose a novel systems-aware optimization method, ATTack on Federated Learning (AT2FL), which is efficiency to derive the implicit gradients for poisoned data, and further compute optimal attack strategies in the federated machine learning. Our work is an earlier study that considers issues of data poisoning attack for federated learning. To the end, experimental results on real-world datasets show that federated multi-task learning model is very sensitive to poisoning attacks, when the attackers either directly poison the target nodes or indirectly poison the related nodes by exploiting the communication protocol.},
	urldate = {2021-12-19},
	journal = {arXiv:2004.10020 [cs]},
	author = {Sun, Gan and Cong, Yang and Dong, Jiahua and Wang, Qiang and Liu, Ji},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.10020},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, I.2.11, I.5},
	file = {arXiv Fulltext PDF:C\:\\Users\\ar7is7\\Zotero\\storage\\2GAWRXRR\\Sun et al. - 2020 - Data Poisoning Attacks on Federated Machine Learni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ar7is7\\Zotero\\storage\\8JI6R4KZ\\2004.html:text/html},
}

@article{jia_intrinsic_2020,
	title = {Intrinsic {Certified} {Robustness} of {Bagging} against {Data} {Poisoning} {Attacks}},
	url = {http://arxiv.org/abs/2008.04495},
	abstract = {In a {\textbackslash}emph\{data poisoning attack\}, an attacker modifies, deletes, and/or inserts some training examples to corrupt the learnt machine learning model. {\textbackslash}emph\{Bootstrap Aggregating (bagging)\} is a well-known ensemble learning method, which trains multiple base models on random subsamples of a training dataset using a base learning algorithm and uses majority vote to predict labels of testing examples. We prove the intrinsic certified robustness of bagging against data poisoning attacks. Specifically, we show that bagging with an arbitrary base learning algorithm provably predicts the same label for a testing example when the number of modified, deleted, and/or inserted training examples is bounded by a threshold. Moreover, we show that our derived threshold is tight if no assumptions on the base learning algorithm are made. We evaluate our method on MNIST and CIFAR10. For instance, our method achieves a certified accuracy of \$91.1{\textbackslash}\%\$ on MNIST when arbitrarily modifying, deleting, and/or inserting 100 training examples. Code is available at: {\textbackslash}url\{https://github.com/jjy1994/BaggingCertifyDataPoisoning\}.},
	urldate = {2021-12-20},
	journal = {arXiv:2008.04495 [cs]},
	author = {Jia, Jinyuan and Cao, Xiaoyu and Gong, Neil Zhenqiang},
	month = dec,
	year = {2020},
	note = {arXiv: 2008.04495},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:C\:\\Users\\ar7is7\\Zotero\\storage\\QCI5FMR3\\Jia et al. - 2020 - Intrinsic Certified Robustness of Bagging against .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ar7is7\\Zotero\\storage\\WVTUE2RW\\2008.html:text/html},
}

@article{HiddenTrigger,
  doi = {10.48550/ARXIV.1910.00033},

  url = {https://arxiv.org/abs/1910.00033},

  author = {Saha, Aniruddha and Subramanya, Akshayvarun and Pirsiavash, Hamed},

  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Hidden Trigger Backdoor Attacks},

  publisher = {arXiv},

  year = {2019},

  copyright = {arXiv.org perpetual, non-exclusive license}
}
